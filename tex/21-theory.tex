\chapter{Теоретическая часть}
\label{cha:theory}

\section{Определения}

\textbf{Корпус текстов} --- множество подобранных и определённым образом обработанных текстов.

\textbf{Токен} --- элементарная единица разбиения корпуса.

\textbf{Токенизация} --- процесс разбиения корпуса на токены с присвоением им уникальных числовых идентификаторов.

\textbf{Языковая модель} --- распределение $P(w_t | w_1,w_2,w_3,\dots,w_n)$ вероятностей встретить токен $w_t$ в корпусе сразу после $n$ токенов $w_i$, $i\in[1, n]$, идущих подряд, где $w_i \in W \, \forall i$, $W$ --- множество всех токенов корпуса, $n$ --- длина контекста модели.

\textbf{Длина контекста} --- количество $n$ токенов $w_i$, $i\in[0, n]$, предшествующих токену $w_t$, от которых зависит вероятность появления в тексте токена $w_t$.

\textbf{Дообучение} --- процесс обучения уже обученной на некоторых данных модели машинного обучения на новых данных. В случае языковой модели это означает подстройку модели под новое распределение токенов.

\textbf{Перплексия} --- мера схожести двух вероятностных распределений, используемая для оценки качества генерации текста языковой моделью. Перплексия задаётся формулой \ref*{eq:perplexity}.
\begin{equation}
    \label{eq:perplexity}
    \textrm{PP}(W)=\sqrt[n]{\frac{1}{P(w_1,w_2,\dots,w_n)}}
\end{equation}

\section{Постановка задачи}

Заданы:
\begin{itemize}
    \item корпус, состоящий из текстов, принадлежащих конкретной предметной области,
    \item предобученная нейросетевая языковая модель, хорошо моделирующая вероятностное распределение слов в естественном языке.
\end{itemize}
Требуется: дообучить данную языковую модель на данных из корпуса, получив новую языковую модель, моделирующую распределение вероятностей слов в данном корпусе, и применить её для генерации новых текстов, принадлежащих предметной области данного корпуса, встроив в графический пользовательский интерфейс.
